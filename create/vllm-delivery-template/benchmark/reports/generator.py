#!/usr/bin/env python3
"""Report generation for vLLM benchmark results.

This module generates human-readable reports in Markdown format
and machine-readable reports in JSON format.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Optional

from jinja2 import Template

from ..metrics.calculator import LatencyStatistics


# Markdown report template
MARKDOWN_TEMPLATE = """# vLLM Benchmark Report

**Generated:** {{ timestamp }}
**Configuration:** {{ config_name }}

---

## Test Configuration

| Parameter | Value |
|-----------|-------|
| Model | `{{ model }}` |
| Server URL | `{{ server_url }}` |
| Concurrency | {{ concurrency }} |
| Total Requests | {{ num_requests }} |
| Input Length (target) | {{ input_length }} tokens |
| Output Length (max) | {{ output_length }} tokens |

---

## Results Summary

| Metric | Value |
|--------|-------|
| **Duration** | {{ duration }}s |
| **Throughput** | {{ throughput }} req/s |
| **Success Rate** | {{ success_rate }}% |
| **Failed Requests** | {{ failed_requests }}/{{ total_requests }} |

---

## Latency Metrics

| Percentile | Latency (ms) |
|------------|--------------|
| Average | {{ avg_latency }}ms |
| Min | {{ min_latency }}ms |
| P50 | {{ p50_latency }}ms |
| P90 | {{ p90_latency }}ms |
| P95 | {{ p95_latency }}ms |
| P99 | {{ p99_latency }}ms |
| Max | {{ max_latency }}ms |
| Std Dev | {{ std_latency }}ms |

---

## Time to First Token (TTFT)

| Percentile | TTFT (ms) |
|------------|-----------|
| Average | {{ avg_ttft }}ms |
| P50 | {{ p50_ttft }}ms |
| P95 | {{ p95_ttft }}ms |
| P99 | {{ p99_ttft }}ms |

---

## Token Throughput

| Metric | Value |
|--------|-------|
| Tokens per Second | {{ tokens_per_second }} |
| Total Output Tokens | {{ total_tokens }} |

---

## Reproduction

To reproduce this benchmark run:

```bash
python benchmark/load_test.py \\
    --config {{ config_path }} \\
    --concurrency {{ concurrency }} \\
    --num-requests {{ num_requests }}
```

---

## Prometheus Metrics

{% if prometheus_metrics %}
{% for key, value in prometheus_metrics.items() %}
| {{ key }} | {{ value }} |
{% endfor %}
{% else %}
*Prometheus metrics not available for this run.*
{% endif %}

---

*Report generated by vLLM Delivery Template - Benchmark Module*
"""


class ReportGenerator:
    """Generate benchmark reports in multiple formats."""

    def __init__(self, template_string: Optional[str] = None):
        """Initialize the report generator.

        Args:
            template_string: Custom Jinja2 template for markdown reports
        """
        self.template = Template(template_string or MARKDOWN_TEMPLATE)

    def generate_markdown(
        self,
        output_path: Path,
        config: dict,
        summary: dict,
        latency_stats: LatencyStatistics,
        ttft_stats: LatencyStatistics,
        prometheus_metrics: Optional[dict] = None,
    ) -> None:
        """Generate a Markdown report.

        Args:
            output_path: Path where the report will be saved
            config: Benchmark configuration dictionary
            summary: Summary statistics (duration, throughput, etc.)
            latency_stats: LatencyStatistics dataclass
            ttft_stats: TTFT LatencyStatistics dataclass
            prometheus_metrics: Optional dict of Prometheus metrics
        """
        server = config.get("server", {})
        load = config.get("load", {})
        prompt = config.get("prompt", {})

        # Render template
        content = self.template.render(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            config_name="default",
            model=server.get("model", "unknown"),
            server_url=server.get("base_url", "unknown"),
            concurrency=load.get("concurrency", 0),
            num_requests=load.get("num_requests", 0),
            input_length=prompt.get("input_length", 0),
            output_length=prompt.get("output_length", 0),
            duration=f"{summary.get('duration_seconds', 0):.2f}",
            throughput=f"{summary.get('throughput_rps', 0):.2f}",
            success_rate=f"{summary.get('success_rate', 0):.1f}",
            failed_requests=summary.get("failed_requests", 0),
            total_requests=summary.get("total_requests", 0),
            # Latency metrics
            avg_latency=f"{latency_stats.avg_ms:.2f}",
            min_latency=f"{latency_stats.min_ms:.2f}",
            p50_latency=f"{latency_stats.p50_ms:.2f}",
            p90_latency=f"{latency_stats.p90_ms:.2f}",
            p95_latency=f"{latency_stats.p95_ms:.2f}",
            p99_latency=f"{latency_stats.p99_ms:.2f}",
            max_latency=f"{latency_stats.max_ms:.2f}",
            std_latency=f"{latency_stats.std_ms:.2f}",
            # TTFT metrics
            avg_ttft=f"{ttft_stats.avg_ms:.2f}",
            p50_ttft=f"{ttft_stats.p50_ms:.2f}",
            p95_ttft=f"{ttft_stats.p95_ms:.2f}",
            p99_ttft=f"{ttft_stats.p99_ms:.2f}",
            # Token throughput
            tokens_per_second=f"{summary.get('tokens_per_second', 0):.2f}",
            total_tokens=summary.get("total_tokens", 0),
            # Config path
            config_path=config.get("_config_path", "benchmark/config/default.yaml"),
            # Prometheus metrics
            prometheus_metrics=prometheus_metrics or {},
        )

        # Write to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            f.write(content)

        print(f"Markdown report saved to: {output_path}")

    def generate_json(
        self,
        output_path: Path,
        config: dict,
        summary: dict,
        latency_stats: LatencyStatistics,
        ttft_stats: LatencyStatistics,
        prometheus_metrics: Optional[dict] = None,
    ) -> None:
        """Generate a JSON report.

        Args:
            output_path: Path where the report will be saved
            config: Benchmark configuration dictionary
            summary: Summary statistics (duration, throughput, etc.)
            latency_stats: LatencyStatistics dataclass
            ttft_stats: TTFT LatencyStatistics dataclass
            prometheus_metrics: Optional dict of Prometheus metrics
        """
        report = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "config": config,
            },
            "summary": summary,
            "latency": {
                "avg_ms": latency_stats.avg_ms,
                "min_ms": latency_stats.min_ms,
                "max_ms": latency_stats.max_ms,
                "std_ms": latency_stats.std_ms,
                "p50_ms": latency_stats.p50_ms,
                "p90_ms": latency_stats.p90_ms,
                "p95_ms": latency_stats.p95_ms,
                "p99_ms": latency_stats.p99_ms,
            },
            "ttft": {
                "avg_ms": ttft_stats.avg_ms,
                "p50_ms": ttft_stats.p50_ms,
                "p95_ms": ttft_stats.p95_ms,
                "p99_ms": ttft_stats.p99_ms,
            },
        }

        if prometheus_metrics:
            report["prometheus_metrics"] = prometheus_metrics

        # Write to file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w") as f:
            json.dump(report, f, indent=2)

        print(f"JSON report saved to: {output_path}")

    def generate_both(
        self,
        output_dir: Path,
        config: dict,
        summary: dict,
        latency_stats: LatencyStatistics,
        ttft_stats: LatencyStatistics,
        prometheus_metrics: Optional[dict] = None,
        timestamp: Optional[str] = None,
    ) -> tuple[Path, Path]:
        """Generate both JSON and Markdown reports.

        Args:
            output_dir: Directory where reports will be saved
            config: Benchmark configuration dictionary
            summary: Summary statistics (duration, throughput, etc.)
            latency_stats: LatencyStatistics dataclass
            ttft_stats: TTFT LatencyStatistics dataclass
            prometheus_metrics: Optional dict of Prometheus metrics
            timestamp: Optional timestamp string for filenames

        Returns:
            Tuple of (json_path, markdown_path)
        """
        if timestamp is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        json_path = output_dir / f"benchmark_{timestamp}.json"
        md_path = output_dir / f"benchmark_{timestamp}.md"

        self.generate_json(
            json_path, config, summary, latency_stats, ttft_stats, prometheus_metrics
        )
        self.generate_markdown(
            md_path, config, summary, latency_stats, ttft_stats, prometheus_metrics
        )

        return json_path, md_path
