#!/bin/bash
# è¿œç¨‹æœåŠ¡å™¨ä¸€é”®éƒ¨ç½²è„šæœ¬
# ç”¨æ³•: ./scripts/deploy_remote.sh [server-alias]
# é»˜è®¤: seeta

set -e

SERVER="${1:-seeta}"
PROJECT_DIR="vllm-delivery-template"
REMOTE_WORK_DIR="/root/$PROJECT_DIR"

echo "=========================================="
echo "  vLLM è¿œç¨‹æœåŠ¡å™¨éƒ¨ç½²è„šæœ¬"
echo "  ç›®æ ‡æœåŠ¡å™¨: $SERVER"
echo "=========================================="
echo ""

# 1. æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
echo "ğŸ“‹ æ£€æŸ¥æœ¬åœ°é¡¹ç›®æ–‡ä»¶..."
if [ ! -f "serving/launch.py" ] || [ ! -f "serving/config.yaml" ]; then
    echo "âŒ é”™è¯¯: è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬"
    exit 1
fi
echo "âœ… æœ¬åœ°æ–‡ä»¶æ£€æŸ¥å®Œæˆ"
echo ""

# 2. æµ‹è¯• SSH è¿æ¥
echo "ğŸ”Œ æµ‹è¯• SSH è¿æ¥..."
if ! ssh -o ConnectTimeout=5 "$SERVER" "echo 'è¿æ¥æˆåŠŸ'" > /dev/null 2>&1; then
    echo "âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨: $SERVER"
    exit 1
fi
echo "âœ… SSH è¿æ¥æ­£å¸¸"
echo ""

# 3. æ£€æŸ¥è¿œç¨‹æœåŠ¡å™¨ç¯å¢ƒ
echo "ğŸ” æ£€æŸ¥è¿œç¨‹æœåŠ¡å™¨ç¯å¢ƒ..."
ssh "$SERVER" << 'ENDSSH'
echo "--- ç³»ç»Ÿä¿¡æ¯ ---"
echo "OS: $(lsb_release -d | cut -f2)"
echo "Python: $(python3 --version)"
echo ""
echo "--- GPU ä¿¡æ¯ ---"
nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader
echo ""

# æ£€æŸ¥ CUDA
if command -v nvcc &> /dev/null; then
    echo "CUDA: $(nvcc --version | grep release | awk '{print $5}' | cut -d',' -f1)"
else
    echo "âš ï¸  CUDA å·¥å…·æœªåœ¨ PATH ä¸­"
fi
ENDSSH
echo ""

# 4. ä¸Šä¼ é¡¹ç›®æ–‡ä»¶
echo "ğŸ“¦ ä¸Šä¼ é¡¹ç›®æ–‡ä»¶åˆ°æœåŠ¡å™¨..."
# åˆ›å»ºè¿œç¨‹ç›®å½•
ssh "$SERVER" "mkdir -p $REMOTE_WORK_DIR"

# åŒæ­¥æ–‡ä»¶ï¼ˆæ’é™¤ .venv, __pycache__, .git ç­‰ï¼‰
rsync -avz --progress \
    --exclude='.venv' \
    --exclude='__pycache__' \
    --exclude='*.pyc' \
    --exclude='.git' \
    --exclude='.pytest_cache' \
    --exclude='*.log' \
    ./ "$SERVER:$REMOTE_WORK_DIR/"

echo "âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ"
echo ""

# 5. è¿œç¨‹å®‰è£…å’Œé…ç½®
echo "ğŸ”§ è¿œç¨‹å®‰è£…ä¾èµ–..."
ssh "$SERVER" << 'ENDSSH'
set -e
cd /root/vllm-delivery-template

echo "--- å®‰è£… uv åŒ…ç®¡ç†å™¨ ---"
if ! command -v uv &> /dev/null; then
    echo "æ­£åœ¨å®‰è£… uv..."
    curl -LsSf https://astral.sh/uv/install.sh | sh
    export PATH="$HOME/.local/bin:$PATH"
fi
echo "uv ç‰ˆæœ¬: $(uv --version)"
echo ""

echo "--- åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ ---"
if [ ! -d ".venv" ]; then
    uv venv
fi
echo ""

echo "--- æ¿€æ´»è™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ– ---"
source .venv/bin/activate
uv pip install -e .
echo ""

echo "--- é…ç½® HuggingFace é•œåƒï¼ˆåŠ é€Ÿæ¨¡å‹ä¸‹è½½ï¼‰---"
mkdir -p ~/.huggingface
cat > ~/.huggingface/endpoints.json << 'EOF'
{
  "hf_endpoint": "https://hf-mirror.com"
}
EOF

echo "--- éªŒè¯å®‰è£… ---"
python3 -c "import vllm; print(f'vLLM ç‰ˆæœ¬: {vllm.__version__}')"
echo ""

echo "âœ… ä¾èµ–å®‰è£…å®Œæˆ"
ENDSSH
echo ""

# 6. åˆ›å»ºå¯åŠ¨è„šæœ¬ï¼ˆåŒ…å«å®Œæ•´ CUDA ç¯å¢ƒå˜é‡ï¼‰
echo "ğŸ“ åˆ›å»ºè¿œç¨‹å¯åŠ¨è„šæœ¬..."
ssh "$SERVER" << 'ENDSSH'
cat > /root/vllm-delivery-template/start_vllm.sh << 'STARTSCRIPT'
#!/bin/bash
# vLLM å¯åŠ¨è„šæœ¬ - å®Œæ•´ç¯å¢ƒå˜é‡é…ç½®
# è¿™æ˜¯è®© V1 engine æ­£å¸¸ç¼–è¯‘å’Œè¿è¡Œçš„å…³é”®é…ç½®

# CUDA è·¯å¾„ï¼ˆgcc ç¼–è¯‘éœ€è¦ï¼‰
export PATH=/usr/local/cuda-12.4/bin:$PATH
export CUDA_HOME=/usr/local/cuda-12.4
export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH
export CPATH=/usr/local/cuda-12.4/include:$CPATH
export LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LIBRARY_PATH

# Triton ç¼“å­˜ï¼ˆæ”¾åœ¨æ•°æ®ç›˜ï¼‰
export TRITON_CACHE_DIR=/root/autodl-tmp/triton_cache
mkdir -p $TRITON_CACHE_DIR

# HuggingFace é…ç½®
export HF_HOME=/root/autodl-tmp/hf_cache
export HF_ENDPOINT=https://hf-mirror.com

# é¡¹ç›®ç›®å½•
cd /root/vllm-delivery-template

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# å¯åŠ¨ vLLM
echo "=== vLLM ç¯å¢ƒé…ç½® ==="
echo "CUDA_HOME: $CUDA_HOME"
echo "TRITON_CACHE_DIR: $TRITON_CACHE_DIR"
echo "HF_HOME: $HF_HOME"
echo "========================"
echo ""

exec vllm serve Qwen/Qwen2.5-3B-Instruct --port 8000 "$@"
STARTSCRIPT

chmod +x /root/vllm-delivery-template/start_vllm.sh
echo "âœ… å¯åŠ¨è„šæœ¬å·²åˆ›å»º: start_vllm.sh"
ENDSSH
echo ""

# 7. åˆ›å»º systemd æœåŠ¡æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
echo "ğŸ“ åˆ›å»º systemd æœåŠ¡é…ç½®..."
ssh "$SERVER" << 'ENDSSH'
sudo tee /etc/systemd/system/vllm.service > /dev/null << 'EOF'
[Unit]
Description=vLLM Inference Server
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
User=root
WorkingDirectory=/root/vllm-delivery-template
Environment="PATH=/root/vllm-delivery-template/.venv/bin:/usr/local/bin:/usr/bin:/bin"
Environment="HF_ENDPOINT=https://hf-mirror.com"
ExecStart=/root/vllm-delivery-template/.venv/bin/python serving/launch.py
Restart=always
RestartSec=10
StandardOutput=append:/root/vllm-delivery-template/logs/vllm-service.log
StandardError=append:/root/vllm-delivery-template/logs/vllm-service-error.log

[Install]
WantedBy=multi-user.target
EOF

echo "âœ… systemd æœåŠ¡æ–‡ä»¶å·²åˆ›å»º"
echo "   å¯ç”¨å‘½ä»¤: sudo systemctl enable vllm"
echo "   å¯åŠ¨å‘½ä»¤: sudo systemctl start vllm"
echo "   çŠ¶æ€æŸ¥çœ‹: sudo systemctl status vllm"
ENDSSH
echo ""

# 8. å®Œæˆ
echo "=========================================="
echo "  âœ… éƒ¨ç½²å®Œæˆï¼"
echo "=========================================="
echo ""
echo "ğŸ“Œ åç»­æ“ä½œï¼š"
echo ""
echo "æ–¹å¼ä¸€: ä½¿ç”¨ tmux ä¿æŒä¼šè¯ï¼ˆæ¨èå¼€å‘æ—¶ä½¿ç”¨ï¼‰"
echo "  ssh $SERVER"
echo "  tmux new -s vllm"
echo "  cd $REMOTE_WORK_DIR && ./start_vllm.sh"
echo "  # Ctrl+B, D åˆ†ç¦»ä¼šè¯"
echo "  # tmux attach -t vllm é‡æ–°è¿æ¥"
echo ""
echo "æ–¹å¼äºŒ: ä½¿ç”¨ systemd æœåŠ¡ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰"
echo "  ssh $SERVER"
echo "  sudo systemctl daemon-reload"
echo "  sudo systemctl enable vllm"
echo "  sudo systemctl start vllm"
echo "  sudo journalctl -u vllm -f  # æŸ¥çœ‹æ—¥å¿—"
echo ""
echo "æ–¹å¼ä¸‰: æ‰‹åŠ¨å¯åŠ¨ï¼ˆä½¿ç”¨å®Œæ•´ç¯å¢ƒå˜é‡ï¼‰"
echo "  ssh $SERVER"
echo "  cd $REMOTE_WORK_DIR"
echo "  ./start_vllm.sh"
echo ""
echo "æ–¹å¼å››: ç›´æ¥è¿è¡Œ vllm å‘½ä»¤ï¼ˆéœ€è¦æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ï¼‰"
echo "  ssh $SERVER"
echo "  cd $REMOTE_WORK_DIR"
echo "  source .venv/bin/activate"
echo "  vllm serve Qwen/Qwen2.5-3B-Instruct --port 8000"
echo ""
echo "ğŸ” å¥åº·æ£€æŸ¥ï¼š"
echo "  curl http://$(ssh $SERVER 'hostname -I | awk \"{print \\$1}\"'):8000/health"
echo ""
echo "ğŸ“Š GPU ç›‘æ§ï¼š"
echo "  ssh $SERVER 'watch -n 1 nvidia-smi'"
echo ""
